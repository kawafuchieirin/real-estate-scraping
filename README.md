# Real Estate Scraping Project / ä¸å‹•ç”£ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

This project scrapes real estate data from major Japanese property portals for data analysis and visualization.

## ğŸ¯ Purpose / ç›®çš„

To gain practical experience in data engineering and data analysis through the complete process of scraping, analyzing, and visualizing real estate data.

ä¸å‹•ç”£ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‹ã‚‰åˆ†æãƒ»å¯è¦–åŒ–ã¾ã§ã‚’ä¸€é€šã‚Šå®Ÿæ–½ã—ã€ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãƒ»ãƒ‡ãƒ¼ã‚¿åˆ†æã‚¹ã‚­ãƒ«ã‚’å®Ÿè·µçš„ã«ç¿’å¾—ã™ã‚‹ã€‚

## ğŸ“‹ Features / æ©Ÿèƒ½

- Scrape property listings from major real estate portals (SUUMO, HOMES)
- Target: Tokyo 23 wards rental properties
- Data export in multiple formats (CSV, JSON, Parquet)
- Data normalization and quality checks
- Geocoding support (Google Maps API or free Nominatim)
- AWS S3 upload integration
- Rate limiting and respectful scraping
- Comprehensive logging with loguru
- Modular architecture for easy extension

## ğŸš€ Quick Start / ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### Prerequisites / å‰ææ¡ä»¶

- Python 3.12+
- pip

### Installation / ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# Clone the repository
git clone https://github.com/kawafuchieirin/real-estate-scraping.git
cd real-estate-scraping

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Basic Usage / åŸºæœ¬çš„ãªä½¿ã„æ–¹

```bash
# Run with default settings (first 3 areas, all property types)
python -m src.main

# Specify areas and property types
python -m src.main --areas 13113 13104 --property-types apartment

# Export as different formats
python -m src.main --export-format json
python -m src.main --export-format parquet

# Limit pages per search
python -m src.main --max-pages 3

# Skip data processing (raw data only)
python -m src.main --skip-processing

# Enable geocoding (requires API key setup)
python -m src.main --geocode

# Upload to S3 (requires AWS credentials)
python -m src.main --upload-s3

# Full example with all features
python -m src.main --sites SUUMO HOMES --areas 13113 --geocode --upload-s3
```

### Command Line Options / ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³

| Option | Description | Default |
|--------|-------------|--------|
| `--sites` | Sites to scrape (SUUMO, HOMES) | All sites |
| `--areas` | Area codes to scrape (e.g., 13113 for Shibuya) | First 3 areas |
| `--property-types` | Property types (apartment, apart, house) | All types |
| `--max-pages` | Maximum pages per search | 5 |
| `--export-format` | Export format (csv, json, parquet) | csv |
| `--skip-processing` | Skip data normalization and quality checks | False |
| `--geocode` | Enable address geocoding | False |
| `--upload-s3` | Upload results to AWS S3 | False |

## ğŸ“ Project Structure / ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

```
real-estate-scraping/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/          # Configuration settings
â”‚   â”œâ”€â”€ scrapers/        # Website scrapers
â”‚   â”œâ”€â”€ models/          # Data models
â”‚   â”œâ”€â”€ utils/           # Utility functions
â”‚   â””â”€â”€ main.py          # Main entry point
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/            # Raw scraped data
â”‚   â””â”€â”€ processed/      # Processed data
â”œâ”€â”€ logs/               # Log files
â”œâ”€â”€ tests/              # Test files
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md          # This file
```

## âš™ï¸ Configuration / è¨­å®š

The project uses the following default settings:

- **Target Areas**: Tokyo 23 wards (æ±äº¬23åŒº)
- **Property Types**: Apartments (ãƒãƒ³ã‚·ãƒ§ãƒ³), Apart (ã‚¢ãƒ‘ãƒ¼ãƒˆ), Houses (ä¸€æˆ¸å»ºã¦)
- **Rate Limiting**: 10 requests per 60 seconds
- **Export Formats**: CSV, JSON, Parquet

Settings can be modified in `src/config/settings.py`.

### Environment Variables / ç’°å¢ƒå¤‰æ•°

For advanced features, create a `.env` file:

```bash
# Geocoding (optional)
GOOGLE_MAPS_API_KEY=your_api_key_here

# AWS S3 Upload (optional)
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
AWS_REGION=ap-northeast-1
S3_BUCKET_NAME=your-bucket-name
```

## ğŸ“Š Data Fields / ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰

Scraped properties include:

- **Basic info**: ID, URL, title, type
- **Location**: Prefecture, city, address, coordinates (with geocoding)
- **Price**: Rent, management fee, deposit, key money
- **Specifications**: Floor plan, area, floor, building age
- **Transportation**: Nearest station, walking distance
- **Features**: Amenities and equipment
- **Metadata**: Scraping timestamp

### Data Processing Features / ãƒ‡ãƒ¼ã‚¿å‡¦ç†æ©Ÿèƒ½

When processing is enabled (default), the system automatically:

- **Normalizes Japanese text**: Converts full-width to half-width characters
- **Standardizes formats**: 
  - Floor plans: `ï¼‘ï¼¬ï¼¤ï¼«` â†’ `1LDK`
  - Rent: `12.5ä¸‡å††` â†’ `125000`
  - Area: `35.00ã¡` â†’ `35.0`
- **Validates data quality**: Detects missing values, outliers, duplicates
- **Generates quality score**: 0-100 score for data completeness
- **Geocodes addresses**: Converts addresses to latitude/longitude (optional)

## ğŸ› ï¸ Development / é–‹ç™º

### Adding New Scrapers / æ–°ã—ã„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®è¿½åŠ 

1. Create a new file in `src/scrapers/`
2. Inherit from `BaseScraper`
3. Implement required methods
4. Add to the scrapers dictionary in `main.py`

### Running Tests / ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ

```bash
pytest tests/
```

### Code Quality / ã‚³ãƒ¼ãƒ‰å“è³ª

```bash
# Format code
black src/

# Check linting
flake8 src/

# Type checking
mypy src/
```

## âš ï¸ Important Notes / é‡è¦ãªæ³¨æ„äº‹é …

1. **Respect robots.txt**: Always check and follow website scraping policies
2. **Rate Limiting**: Built-in rate limiting prevents overwhelming target servers
3. **Legal Compliance**: Ensure compliance with website terms of service
4. **Data Privacy**: Handle scraped data responsibly

## ğŸ—ºï¸ Roadmap / ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

- [x] Step 1: Basic scraping implementation
- [ ] Step 2: Full scraping implementation for multiple sites
- [ ] Step 3: Data cleaning and normalization
- [ ] Step 4: AWS integration (S3, Glue, Athena)
- [ ] Step 5: Data visualization (QuickSight)
- [ ] Step 6: Machine learning models

## ğŸ“„ License / ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

This project is for educational purposes. Please ensure compliance with all relevant laws and website terms of service when using.

## ğŸ¤ Contributing / ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³

Contributions are welcome! Please feel free to submit a Pull Request
